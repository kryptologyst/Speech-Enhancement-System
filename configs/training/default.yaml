# @package training
_target_: src.train.trainer.Trainer

# Training parameters
epochs: 100
batch_size: 32
learning_rate: 1e-3
weight_decay: 1e-5
gradient_clip_norm: 1.0

# Optimizer
optimizer: adam
scheduler: cosine_annealing

# Loss function
loss_type: mse

# Validation
val_frequency: 1
save_frequency: 10
early_stopping_patience: 20

# Logging
log_frequency: 10
tensorboard: true
wandb: false
